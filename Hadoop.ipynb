{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data science thats beyond the scope of a local computer using Spark for Big Data.\n",
    "\n",
    "# Hadoop | MapReduce | Spark | PySpark | Local Vs Distributed Systems | Hadoop Ecosystem | AWS | \n",
    "\n",
    "Beyond a point, to scale up the data, it is much easier to scale out to many lower CPU machines than to try to scale up to \n",
    "a single machine with high CPU. Distributed systems can easily scale up by adding more machines. And if one node fails others work fine and hence it is fault tolerant system.  Local computer makes its harder to upgrade CPU and memory and eventually you are going to reach a limit very soon. They are not fault tolerant.\n",
    "\n",
    "Distributed Architecture using Hadoop:\n",
    "\n",
    "Hadoop is a way to distribute vey large files across multiple machines. It uses Hadoop distributed file system. User can work with large datasets. It duplicates blocks of data for fault tolerance. It uses MapReduce which allows computations on the distributed data.\n",
    "\n",
    "(Main) NameNode(CPU,RAM) -----------------DataNode(CPU,RAM)(Lower)\n",
    "\n",
    "                         -----------DataNode(CPU,RAM)\n",
    "                         \n",
    "                         -----------DataNode(CPU,RAM)\n",
    "                         \n",
    "                                    Distributed Machines\n",
    "\n",
    "HDFS  uses 128 MB default data blocks replicated among the distributed machines to support fault tolerance.\n",
    "smaller blocks provide more parallelization during processing\n",
    "multiple copies prevent loss of data due to node failure\n",
    "\n",
    "MapReduce is a way of splitting a computation task to a distribution set of files (like HDFS)\n",
    "\n",
    "Job Tracker(Sends code to run on the task trackers) ----------------------------------Multiple Task Trackers(Task trackers allocate CPU and memory for the tasks and monitor the tasks on the worker nodes)\n",
    "                        \n",
    "\n",
    "*Using HDFS to  distribute large data sets\n",
    "*Using MapReduce to distribute a computational task to a distributed data set. \n",
    "\n",
    "Spark improves on the conceps of using distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "Spark Ecosystem | Spark vs MapReduce | Spark RDDs | RDD Opeartions \n",
    "\n",
    "Spark is for quickly and easily handling Big Data. Open source project on Apache. Easy to use and Fast. Acts as flexible alternative to MapReduce. It  can run on top of existing HDFS infrastructure to provide and enhace additional functionalities. \n",
    "It can also use data stored in Cassandra, Amazon s3 storage etc. \n",
    "\n",
    "It should not be seen as replacement for Hadoop. it is an alternative for MapReduce.  It is  for providing comprehensive and unified solution to manage different big data use cases and requirements. MapReduce needs files to be stored in HDFS while spark does not lthough it can run on top of HDFS. Spark can go to 100x speeds than MapReduce. \n",
    "\n",
    "MapReduce writes most data to Disk after each map and Reduce Operation.\n",
    "Spark keeps most of it in memory after each transformation, can spill over to disk if memory is filled. \n",
    "\n",
    "\n",
    "At the core of spark, is the idea of Resilient Distributed Dataset.\n",
    "Features: \n",
    "Distributed Collection of Data\n",
    "Fault-tolerant\n",
    "Parallel operation- Partitioned data\n",
    "Ability to use many data sources\n",
    "\n",
    "Driver Program(operates a Spark context)------ communicates with a cluster manager----- Communicates with worker nodes(Executor, cache, task)- execute various tasks or computations asked for on that distributed data\n",
    "\n",
    "Spark allows Programmers to develop complex multistep data pipelines using directed acyclic graph patterns. Supports in memory data steering across these DAGs so different jobs can work with same data. \n",
    "Our work im Spark with Python-- Concerns with RDD objects. Evrything else occurs under the hood for us. \n",
    "RDDs are immutable, lazily evaluated and cacheable. \n",
    "\n",
    "2 RDD operations----- Transformations and actions-- core of what we will do -- to code transformations and actions on some sort if distributed dataset. \n",
    "\n",
    "Basic Actions: these are like methods when we code in python on RDD object we create: First(Return the firt element in the RDD), Collect(Return all the elements of the RDD as an array at the driver program), Count(Return the number of elements in the RDD), Take(Return an array with first n elements of the RDD)\n",
    "\n",
    "Tranformations--Filter, Map, FlatMap\n",
    "RDD.filter()- applies a function to each element and returns the elements that evaluate to true\n",
    "RDD.map()- transforms each element and preserves same number of elements. like pandas.apply()\n",
    "RDD.flatmap()- transforms each element to 0-N elements and will probably change number of elements\n",
    "All these are programmed using PySpark\n",
    " \n",
    "Pair RDDs\n",
    "Often RDDs will be holding values in key value pairs(tuple). Which is better partioning of data and leads to functionality based off of reduction. \n",
    "\n",
    "Reduce()-- Action that will aggregate RDD elements using a function that returns single element\n",
    "\n",
    "ReduceByKey()--Action that will agggregate pair RDD elements using  a fuction that returns a pair RDD\n",
    "which are similar to groupby operations. \n",
    "\n",
    "Learn to use spark with some sort of  distributed set of machines that you access through some sort of cloud services and not a local computer(AWS is goodplatform choice- popular cloud services so far.)\n",
    "If on local computer, u may use SQL or Pandas just like locall computer low memory low processing power uses.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS CLoud services\n",
    "we will be using Amazon EC2. Its like  a cloud computer like a virtual computer.\n",
    "Amazon Elastic compute cloud(EC2) is a web service that provides resizable compute capacity in the cloud. in laymans terms this is a virtual computer thats being accessed through the internet. \n",
    "Create aws console to create an EC2 instance\n",
    "use SSH/secure shell connection to connect to EC2 instance over internet.\n",
    "our goal is to remotely connect to command line of virtual machine on Amazon EC2\n",
    "putty mobaxterm are going to allow us to establish a secure shell(ssh ) connection to our ec2 instance.\n",
    "Enable inbound ssh traffic from ur ip address to your instance (when u selected all traffic when u created  the instance)\n",
    "When we set up the EC2 instance and configure the security groups setting for these spot instances we keep all the ports are open for simplicity, but it should be noted that these settings should be much more strict if put in production. So keep that in mind if you want to use the concepts discussed in the next lecture for a formal production environment.\n",
    "Setting up spark, hADoop and jupyter notebooks all on our virtual instance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark is written in scala. so we need to install scala\n",
    "scala depends on java. so we need to install java\n",
    "jupyter notebooks\n",
    "Python\n",
    "Anaconda\n",
    "Java\n",
    "Py4j\n",
    "Spark\n",
    "Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
